{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "bilstm_geotrendbert_nerc.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install Library"
      ],
      "metadata": {
        "id": "WNifY87Ma90h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "6l12BHk3a6B2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Library"
      ],
      "metadata": {
        "id": "XaVBg2vLYNIm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "import sklearn\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from itertools import chain"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-25T01:31:52.259319Z",
          "iopub.execute_input": "2022-02-25T01:31:52.259762Z",
          "iopub.status.idle": "2022-02-25T01:31:52.287111Z",
          "shell.execute_reply.started": "2022-02-25T01:31:52.259642Z",
          "shell.execute_reply": "2022-02-25T01:31:52.286300Z"
        },
        "trusted": true,
        "id": "uh13Znzo0DXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model, Model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Conv1D, Input\n",
        "from keras.layers import Bidirectional, concatenate, SpatialDropout1D, GlobalMaxPooling1D\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-24T15:10:47.578814Z",
          "iopub.execute_input": "2022-02-24T15:10:47.579059Z",
          "iopub.status.idle": "2022-02-24T15:10:47.606003Z",
          "shell.execute_reply.started": "2022-02-24T15:10:47.579032Z",
          "shell.execute_reply": "2022-02-24T15:10:47.605368Z"
        },
        "trusted": true,
        "id": "ORYOVZw70DYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Data for Bert"
      ],
      "metadata": {
        "id": "-aAfMLWCYRzj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#LST20 data\n",
        "with open('train_sent_lst20.data', 'rb') as filehandle:\n",
        "    train_sent = pickle.load(filehandle)\n",
        "with open('train_ner_lst20.data', 'rb') as filehandle:\n",
        "    train_ner = pickle.load(filehandle)\n",
        "with open('eval_sent_lst20.data', 'rb') as filehandle:\n",
        "    eval_sent = pickle.load(filehandle)\n",
        "with open('eval_ner_lst20.data', 'rb') as filehandle:\n",
        "    eval_ner = pickle.load(filehandle)"
      ],
      "metadata": {
        "id": "yW0KfKLgYPzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ner_tags = [\n",
        "        \"O\",\n",
        "        \"B_BRN\",\n",
        "        \"B_DES\",\n",
        "        \"B_DTM\",\n",
        "        \"B_LOC\",\n",
        "        \"B_MEA\",\n",
        "        \"B_NUM\",\n",
        "        \"B_ORG\",\n",
        "        \"B_PER\",\n",
        "        \"B_TRM\",\n",
        "        \"B_TTL\",\n",
        "        \"I_BRN\",\n",
        "        \"I_DES\",\n",
        "        \"I_DTM\",\n",
        "        \"I_LOC\",\n",
        "        \"I_MEA\",\n",
        "        \"I_NUM\",\n",
        "        \"I_ORG\",\n",
        "        \"I_PER\",\n",
        "        \"I_TRM\",\n",
        "        \"I_TTL\",\n",
        "        \"E_BRN\",\n",
        "        \"E_DES\",\n",
        "        \"E_DTM\",\n",
        "        \"E_LOC\",\n",
        "        \"E_MEA\",\n",
        "        \"E_NUM\",\n",
        "        \"E_ORG\",\n",
        "        \"E_PER\",\n",
        "        \"E_TRM\",\n",
        "        \"E_TTL\",\n",
        "    ]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-25T01:31:53.624495Z",
          "iopub.execute_input": "2022-02-25T01:31:53.625197Z",
          "iopub.status.idle": "2022-02-25T01:31:53.630426Z",
          "shell.execute_reply.started": "2022-02-25T01:31:53.625159Z",
          "shell.execute_reply": "2022-02-25T01:31:53.629390Z"
        },
        "trusted": true,
        "id": "_y52nCMg0DXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dump=[]\n",
        "for i, ner_sent in enumerate(train_ner):\n",
        "    for ner in ner_sent:\n",
        "        if not (ner in ner_tags):\n",
        "            dump.append(ner_sent)\n",
        "            break\n",
        "for ners in dump:\n",
        "    idx = train_ner.index(ners)\n",
        "    train_ner.pop(idx)\n",
        "    train_sent.pop(idx)    \n",
        "\n",
        "#limit 300 words\n",
        "for i, item in enumerate(train_sent):\n",
        "    if len(item)>300:\n",
        "        train_sent[i]=item[:300]\n",
        "        train_ner[i]=train_ner[i][:300]\n",
        "train_toks = []"
      ],
      "metadata": {
        "id": "uY560EEzYfZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_toks = []\n",
        "for sent in train_sent:\n",
        "    train_toks.append(' '.join(sent))"
      ],
      "metadata": {
        "id": "hf6NiIPMZX0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "seq_len = 300\n",
        "num_samples = len(train_toks)\n",
        "\n",
        "Xids = np.zeros((num_samples, seq_len))\n",
        "Xmask = np.zeros((num_samples, seq_len))"
      ],
      "metadata": {
        "id": "i035O3ZzZbM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"Geotrend/bert-base-en-th-cased\")\n",
        "\n",
        "for i, phrase in enumerate(train_toks):\n",
        "    tokens = tokenizer.encode_plus(phrase, max_length=seq_len, truncation=True, padding='max_length',\n",
        "                                   add_special_tokens=True, return_tensors='tf')\n",
        "    Xids[i,:] = tokens['input_ids']\n",
        "    Xmask[i,:] =tokens['attention_mask']"
      ],
      "metadata": {
        "id": "cy4W3GSYZoQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ner_tags_sorted = sorted(ner_tags)\n",
        "ner_tags_sorted.append('pad')\n",
        "ner_to_ix = dict((c, i) for i, c in enumerate(ner_tags_sorted))\n",
        "ix_to_ner = dict((v,k) for k,v in ner_to_ix.items()) "
      ],
      "metadata": {
        "id": "lFsQqavuZ2R2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_sequence_target(input_label):\n",
        "    idxs = list()\n",
        "    for word in input_label:\n",
        "        if word in ner_to_ix.keys():\n",
        "            idxs.append(ner_to_ix[word])\n",
        "        else:\n",
        "            idxs.append(ner_to_ix[\"O\"])\n",
        "    return idxs"
      ],
      "metadata": {
        "id": "utVW_l0zZ5Nf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot(labels, n_label):\n",
        "    out = np.zeros((len(labels), n_label))\n",
        "    for i, item in enumerate(labels):\n",
        "        one_hot= np.zeros(n_label)\n",
        "        one_hot[item] = 1\n",
        "        out[i,:]=one_hot\n",
        "    return out"
      ],
      "metadata": {
        "id": "-RAAnAEHaE-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = y_tr = [prepare_sequence_target(s) for s in train_ner]\n",
        "labels = pad_sequences(maxlen=seq_len, sequences=labels, value=ner_to_ix[\"pad\"], padding='post', truncating='post')"
      ],
      "metadata": {
        "id": "g2gHeDp-aG2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_arr = np.zeros((len(train_ner), seq_len, len(ner_tags_sorted)))"
      ],
      "metadata": {
        "id": "_x7hDbMdaJE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, item in enumerate(labels):\n",
        "    labels_arr[i,:,:]= one_hot(item,32)\n",
        "\n",
        "labels_arr = labels_arr.astype('uint8')"
      ],
      "metadata": {
        "id": "sQA7kOqyaYON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ix_to_ner = dict((i, c) for i, c in enumerate(sorted(ner_tags)+['pad']))\n",
        "ner_to_ix = dict((v,k) for k,v in ix_to_ner.items())"
      ],
      "metadata": {
        "id": "UzWk7zetYfoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ner_tags_sorted = sorted(ner_tags)\n",
        "ner_tags_sorted.append('pad')\n",
        "ner_to_ix = dict((c, i) for i, c in enumerate(ner_tags_sorted))\n",
        "ix_to_ner = dict((v,k) for k,v in ner_to_ix.items()) "
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-24T22:15:03.362020Z",
          "iopub.execute_input": "2022-02-24T22:15:03.362296Z",
          "iopub.status.idle": "2022-02-24T22:15:03.369425Z",
          "shell.execute_reply.started": "2022-02-24T22:15:03.362259Z",
          "shell.execute_reply": "2022-02-24T22:15:03.368726Z"
        },
        "trusted": true,
        "id": "mQMbvkpp0DYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((Xids, Xmask, labels_arr))\n",
        "dataset.take(1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-24T22:15:07.431504Z",
          "iopub.execute_input": "2022-02-24T22:15:07.431708Z",
          "iopub.status.idle": "2022-02-24T22:15:11.578938Z",
          "shell.execute_reply.started": "2022-02-24T22:15:07.431683Z",
          "shell.execute_reply": "2022-02-24T22:15:11.576729Z"
        },
        "trusted": true,
        "id": "kjOiCADb0DYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def map_func(input_ids, masks, labels):\n",
        "    return {'input_ids':input_ids, 'attention_mask':masks}, labels"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-24T22:15:11.581968Z",
          "iopub.execute_input": "2022-02-24T22:15:11.582445Z",
          "iopub.status.idle": "2022-02-24T22:15:11.586780Z",
          "shell.execute_reply.started": "2022-02-24T22:15:11.582404Z",
          "shell.execute_reply": "2022-02-24T22:15:11.586062Z"
        },
        "trusted": true,
        "id": "tprTSjzN0DYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.map(map_func)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-24T22:15:11.588348Z",
          "iopub.execute_input": "2022-02-24T22:15:11.588659Z",
          "iopub.status.idle": "2022-02-24T22:15:11.651299Z",
          "shell.execute_reply.started": "2022-02-24T22:15:11.588623Z",
          "shell.execute_reply": "2022-02-24T22:15:11.650624Z"
        },
        "trusted": true,
        "id": "yBBElCu_0DYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "dataset = dataset.shuffle(10000).batch(batch_size, drop_remainder=True)\n",
        "\n",
        "dataset.take(1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-24T22:15:11.652674Z",
          "iopub.execute_input": "2022-02-24T22:15:11.652921Z",
          "iopub.status.idle": "2022-02-24T22:15:11.667189Z",
          "shell.execute_reply.started": "2022-02-24T22:15:11.652888Z",
          "shell.execute_reply": "2022-02-24T22:15:11.666573Z"
        },
        "trusted": true,
        "id": "QvzFNyiE0DYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split = 0.95\n",
        "\n",
        "size = int((num_samples/batch_size)*split)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-24T22:15:11.668410Z",
          "iopub.execute_input": "2022-02-24T22:15:11.668773Z",
          "iopub.status.idle": "2022-02-24T22:15:11.673265Z",
          "shell.execute_reply.started": "2022-02-24T22:15:11.668738Z",
          "shell.execute_reply": "2022-02-24T22:15:11.672363Z"
        },
        "trusted": true,
        "id": "UGtvHHkn0DYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = dataset.take(size)\n",
        "val_ds = dataset.skip(size)\n",
        "\n",
        "del dataset"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-24T22:15:11.674680Z",
          "iopub.execute_input": "2022-02-24T22:15:11.675274Z",
          "iopub.status.idle": "2022-02-24T22:15:11.683063Z",
          "shell.execute_reply.started": "2022-02-24T22:15:11.675211Z",
          "shell.execute_reply": "2022-02-24T22:15:11.682272Z"
        },
        "trusted": true,
        "id": "C-XlOael0DYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model building"
      ],
      "metadata": {
        "id": "XkpMhyScbekD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFAutoModel\n",
        "\n",
        "bert = TFAutoModel.from_pretrained('Geotrend/bert-base-en-th-cased')\n",
        "\n",
        "bert.summary()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-24T22:15:16.662296Z",
          "iopub.execute_input": "2022-02-24T22:15:16.662877Z",
          "iopub.status.idle": "2022-02-24T22:15:43.153363Z",
          "shell.execute_reply.started": "2022-02-24T22:15:16.662835Z",
          "shell.execute_reply": "2022-02-24T22:15:43.151932Z"
        },
        "trusted": true,
        "id": "7copWCBU0DYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_lstm_unit = 256 ## Bidirectional 256 + 256 = 512\n",
        "lstm_recurrent_dropout = 0.5"
      ],
      "metadata": {
        "id": "RwAsu2wp3aeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids =  Input(shape=(seq_len,), name='input_ids', dtype='int32')\n",
        "masks =  Input(shape=(seq_len,), name='attention_mask', dtype='int32')\n",
        "\n",
        "embeddings = bert.bert(input_ids, attention_mask=masks)[0]\n",
        "\n",
        "word_embeddings = SpatialDropout1D(0.3)(embeddings)\n",
        "\n",
        "# BiLSTM\n",
        "main_lstm = Bidirectional(LSTM(units=main_lstm_unit, return_sequences=True,\n",
        "                               recurrent_dropout=lstm_recurrent_dropout))(word_embeddings)\n",
        "main_lstm = TimeDistributed(Dense(50, activation=\"relu\"))(main_lstm)\n",
        "\n",
        "out = Dense(32, activation='softmax', name='output')(main_lstm)"
      ],
      "metadata": {
        "id": "m1F6_KlG0DYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(inputs=[input_ids, masks], outputs=out)\n",
        "\n",
        "model.layers[2].trainable = False\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "U6yqNci00DYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "WLKo9HWMdCV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001, decay=1e-6), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "early_stopping= EarlyStopping(monitor='loss', min_delta=0, patience=5, verbose=0, mode='min')\n",
        "\n",
        "filepath=\"bert_bilstm_best_weight.h5\"\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "\n",
        "callbacks_list = [early_stopping,checkpoint]"
      ],
      "metadata": {
        "id": "ilfENrah0DYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_ds, epochs=15, verbose=1,callbacks=callbacks_list, validation_data = val_ds)"
      ],
      "metadata": {
        "id": "GoLPPm5z4_cm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction and Evaluation"
      ],
      "metadata": {
        "id": "Uau7xw0BdFXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_ev = [prepare_sequence_target(s) for s in eval_ner]\n",
        "y_ev = pad_sequences(maxlen=seq_len, sequences=y_ev, value=ner_to_ix[\"pad\"], padding='post', truncating='post')\n",
        "y_ev = [to_categorical(i, num_classes=len(ix_to_ner)) for i in y_ev]"
      ],
      "metadata": {
        "id": "r4A2_t8oYTiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('Geotrend/bert-base-en-th-cased')\n",
        "\n",
        "def prep_data(text):\n",
        "  tokens = tokenizer.encode_plus(text, max_length=300, truncation=True, padding='max_length', add_special_tokens=True, \n",
        "                                 return_token_type_ids=False, return_tensors='tf')\n",
        "  return {\n",
        "      'input_ids' : tf.cast(tokens['input_ids'], tf.float64),\n",
        "      'attention_mask' : tf.cast(tokens['attention_mask'], tf.float64)\n",
        "  }"
      ],
      "metadata": {
        "id": "QXx1U4oi5UHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = []\n",
        "for sent in eval_sent:\n",
        "  pred.append(model.predict(prep_data(sent)).squeeze())"
      ],
      "metadata": {
        "id": "N0mKUx4T7Let"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "y_pred = []\n",
        "y_true = []\n",
        "\n",
        "for i in range(0,len(pred)):\n",
        "  true = np.argmax(y_ev[i], axis=-1)\n",
        "  revert_pred=[ix_to_ner[pred[i][j,:].argmax()] for j in range(pred[i].shape[0])]\n",
        "\n",
        "  revert_true=[ix_to_ner[i] for i in true]\n",
        "  y_pred.append(revert_pred)\n",
        "  y_true.append(revert_true)"
      ],
      "metadata": {
        "id": "0D-twf1TggmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ner_classification_report(y_true, y_pred):\n",
        " \n",
        "    lb = LabelBinarizer()\n",
        "    y_true_combined = lb.fit_transform(list(chain.from_iterable(y_true)))\n",
        "    y_pred_combined = lb.transform(list(chain.from_iterable(y_pred)))\n",
        "    tagset = list(sorted(set(lb.classes_)))\n",
        "    tagset = tagset[:-2]\n",
        "    print(tagset)\n",
        "    class_indices = {cls: idx for idx, cls in enumerate(lb.classes_)}\n",
        "    \n",
        "    return classification_report(\n",
        "        y_true_combined,\n",
        "        y_pred_combined,\n",
        "        labels = [class_indices[cls] for cls in tagset],\n",
        "        target_names = tagset,\n",
        "        digits=4\n",
        "    )"
      ],
      "metadata": {
        "id": "orLhLURfz70Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ner_classification_report(y_true,y_pred))\n"
      ],
      "metadata": {
        "id": "e8WzUO4knzSe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}